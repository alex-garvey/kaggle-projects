{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Resources\n# https://keras.io/examples/nlp/pretrained_word_embeddings/\n# https://nlp.stanford.edu/projects/glove/\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nfrom tensorflow.keras.layers import Embedding\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-27T18:53:32.799655Z","iopub.execute_input":"2023-06-27T18:53:32.800023Z","iopub.status.idle":"2023-06-27T18:53:32.808525Z","shell.execute_reply.started":"2023-06-27T18:53:32.799994Z","shell.execute_reply":"2023-06-27T18:53:32.807252Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# Some pre-processing as suggested by https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n# I suspect that the ruby file does not work as intended, hence this approach.\ndef preprocess(input_df):\n    eyes = \"[8:=;]\"\n    nose = \"['`\\-]?\"\n    input_df.replace(to_replace=\"((http[s]?|ftp):\\/)?\\/?([^:\\/\\s]+)((\\/\\w+)*\\/)([\\w\\-\\.]+[^#?\\s]+)(.*)?(#[\\w\\-]+)?\", value=\"<URL>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"@\\w+\", value=\"<USER>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"\"+eyes+nose+\"[)d]+|[)d]+\"+eyes+nose+\"\", value=\"<SMILE>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"\"+eyes+nose+\"[pP]+\", value=\"<LOLFACE>\", regex=True, inplace=True) # improved, added [pP] instead of p. Could this have a negative effect?\n    input_df.replace(to_replace=\"\"+eyes+nose+\"\\(+|\\)+\"+nose+eyes, value=\"<SADFACE>\", regex=True, inplace=True)\n    input_df.replace(to_replace=eyes+nose+\"[\\/|l*]\", value=\"<NEUTRALFACE>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"<3\", value=\"<HEART>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", value=\"<NUMBER>\", regex=True, inplace=True)\n    input_df.replace(to_replace=\"#\", value=\"\", regex=True, inplace=True) \n    input_df.replace(to_replace=\"\\n\", value=\" \", regex=True, inplace=True)\n    return\n\npreprocess(train_df)\npreprocess(test_df)\n\nsamples = train_df[\"text\"].tolist()\nlabels = train_df[\"target\"].tolist()\ntest_samples = test_df[\"text\"].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:48:30.158861Z","iopub.execute_input":"2023-06-27T18:48:30.159275Z","iopub.status.idle":"2023-06-27T18:48:31.283907Z","shell.execute_reply.started":"2023-06-27T18:48:30.159245Z","shell.execute_reply":"2023-06-27T18:48:31.282734Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# This boolean allow us to train only on a portion of the train set, while using part as a validation set. \n# This is turned off before submission in order to use the most data while training because the set size is small.\nuse_dev_set = False\nif use_dev_set:\n    # Shuffle the data\n    seed = 1337\n    rng = np.random.RandomState(seed)\n    rng.shuffle(samples)\n    rng = np.random.RandomState(seed)\n    rng.shuffle(labels)\n\n    # Extract a training & validation split\n    validation_split = 0.2\n    num_validation_samples = int(validation_split * len(samples))\n\n    train_samples = samples[:-num_validation_samples]\n    val_samples = samples[-num_validation_samples:]\n    train_labels = labels[:-num_validation_samples]\n    val_labels = labels[-num_validation_samples:]\nelse:\n    train_samples = samples\n    val_samples = []\n    train_labels = labels\n    val_labels = []\n    \n\nprint(\"Train set size: \" + str(len(train_samples)))\nprint(\"Dev set size: \" + str(len(val_samples)))","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:48:35.328724Z","iopub.execute_input":"2023-06-27T18:48:35.329150Z","iopub.status.idle":"2023-06-27T18:48:35.336203Z","shell.execute_reply.started":"2023-06-27T18:48:35.329123Z","shell.execute_reply":"2023-06-27T18:48:35.335228Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Train set size: 7613\nDev set size: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Get the maximum length of the input text in words\nmaxlen = len(max(samples, key=lambda x: len(x.split())).split())\n\nprint(\"Longest string contains \" + str(maxlen) + \" words\")","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:48:39.324300Z","iopub.execute_input":"2023-06-27T18:48:39.324814Z","iopub.status.idle":"2023-06-27T18:48:39.343682Z","shell.execute_reply.started":"2023-06-27T18:48:39.324773Z","shell.execute_reply":"2023-06-27T18:48:39.341806Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Longest string contains 31 words\n","output_type":"stream"}]},{"cell_type":"code","source":"# We index the vocabulary found in the dataset. The output will be padded/truncated to exactly output_sequence_length values, resulting\n# in a tensor with shape [batch_size, output_sequence_length]\n\nvectorizer = TextVectorization(max_tokens=20000, output_sequence_length=maxlen)\ntext_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(128)\nvectorizer.adapt(text_ds)\n\n# Get a dictionary mapping words to indices\nvoc = vectorizer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:54:40.738555Z","iopub.execute_input":"2023-06-27T18:54:40.739970Z","iopub.status.idle":"2023-06-27T18:54:41.000149Z","shell.execute_reply.started":"2023-06-27T18:54:40.739942Z","shell.execute_reply":"2023-06-27T18:54:40.999465Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained word embeddings (trained on tweets!)\n!wget -nc https://nlp.stanford.edu/data/glove.twitter.27B.zip\n!unzip -n -q glove.twitter.27B.zip","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:55:45.563656Z","iopub.execute_input":"2023-06-27T18:55:45.564063Z","iopub.status.idle":"2023-06-27T18:55:46.173036Z","shell.execute_reply.started":"2023-06-27T18:55:45.564034Z","shell.execute_reply":"2023-06-27T18:55:46.171592Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"File ‘glove.twitter.27B.zip’ already there; not retrieving.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# The zip offers multiple options (25, 50, 100, 200) choose which word vector dimensions we want.\nzip_dim = 25\npath_to_glove_file = \"/kaggle/working/glove.twitter.27B.\" + str(zip_dim) + \"d.txt\"","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:55:46.386705Z","iopub.execute_input":"2023-06-27T18:55:46.387471Z","iopub.status.idle":"2023-06-27T18:55:46.395339Z","shell.execute_reply.started":"2023-06-27T18:55:46.387430Z","shell.execute_reply":"2023-06-27T18:55:46.393798Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1) \n        coefs = np.fromstring(coefs, \"f\", sep=\" \") # Get vector as 1-D array of floats\n        embeddings_index[word] = coefs # Store each word vector in a dictionary (word:coefs)\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:56:40.200277Z","iopub.execute_input":"2023-06-27T18:56:40.200676Z","iopub.status.idle":"2023-06-27T18:56:47.253109Z","shell.execute_reply.started":"2023-06-27T18:56:40.200647Z","shell.execute_reply":"2023-06-27T18:56:47.251137Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Found 1193514 word vectors.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prepare embedding matrix\nnum_tokens = len(voc) + 2\nembedding_dim = zip_dim\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n\nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n\n# Some examples of misses:\n# bioterror prebreak typhoondevastated bestnaijamade bioterrorism soudelor\n","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:56:47.255407Z","iopub.execute_input":"2023-06-27T18:56:47.256080Z","iopub.status.idle":"2023-06-27T18:56:47.299636Z","shell.execute_reply.started":"2023-06-27T18:56:47.256055Z","shell.execute_reply":"2023-06-27T18:56:47.298379Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Converted 11913 words (2649 misses)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create the embedding layer with trainable=False\nembedding_layer = Embedding(\n    num_tokens, # number of different words in the training set + 2\n    embedding_dim, # dimension of the word vectors\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix), # the matrix we just created!\n    trainable=False, # keep embeddings fixed\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:57:33.576382Z","iopub.execute_input":"2023-06-27T18:57:33.576748Z","iopub.status.idle":"2023-06-27T18:57:33.587837Z","shell.execute_reply.started":"2023-06-27T18:57:33.576722Z","shell.execute_reply":"2023-06-27T18:57:33.586275Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Build the model\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = layers.LSTM(units=128, return_sequences=False)(embedded_sequences)\nx = layers.Dropout(rate=0.5)(x)\npreds = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs=int_sequences_input, outputs=preds)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:57:33.860780Z","iopub.execute_input":"2023-06-27T18:57:33.861176Z","iopub.status.idle":"2023-06-27T18:57:34.266666Z","shell.execute_reply.started":"2023-06-27T18:57:33.861147Z","shell.execute_reply":"2023-06-27T18:57:34.265192Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, None)]            0         \n                                                                 \n embedding (Embedding)       (None, None, 25)          364100    \n                                                                 \n lstm (LSTM)                 (None, 128)               78848     \n                                                                 \n dropout (Dropout)           (None, 128)               0         \n                                                                 \n dense (Dense)               (None, 1)                 129       \n                                                                 \n=================================================================\nTotal params: 443,077\nTrainable params: 78,977\nNon-trainable params: 364,100\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\n\nx_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\ny_train = np.array(train_labels)\n\nif use_dev_set:\n    x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n    y_val = np.array(val_labels)\n\n\nmodel.compile(\n    loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=['accuracy']\n)\n\nepochs = 20\nbatch_size = 128\nif use_dev_set:\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\nelse:\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-27T18:57:36.418664Z","iopub.execute_input":"2023-06-27T18:57:36.419034Z","iopub.status.idle":"2023-06-27T18:59:00.445654Z","shell.execute_reply.started":"2023-06-27T18:57:36.419005Z","shell.execute_reply":"2023-06-27T18:59:00.443839Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch 1/20\n60/60 [==============================] - 6s 56ms/step - loss: 0.5732 - accuracy: 0.7012\nEpoch 2/20\n60/60 [==============================] - 3s 56ms/step - loss: 0.4869 - accuracy: 0.7805\nEpoch 3/20\n60/60 [==============================] - 3s 54ms/step - loss: 0.4671 - accuracy: 0.7869\nEpoch 4/20\n60/60 [==============================] - 3s 58ms/step - loss: 0.4589 - accuracy: 0.7908\nEpoch 5/20\n60/60 [==============================] - 3s 56ms/step - loss: 0.4499 - accuracy: 0.7942\nEpoch 6/20\n60/60 [==============================] - 3s 57ms/step - loss: 0.4440 - accuracy: 0.7994\nEpoch 7/20\n60/60 [==============================] - 4s 61ms/step - loss: 0.4499 - accuracy: 0.7982\nEpoch 8/20\n60/60 [==============================] - 4s 62ms/step - loss: 0.4340 - accuracy: 0.8056\nEpoch 9/20\n60/60 [==============================] - 4s 59ms/step - loss: 0.4296 - accuracy: 0.8069\nEpoch 10/20\n60/60 [==============================] - 3s 58ms/step - loss: 0.4301 - accuracy: 0.8070\nEpoch 11/20\n60/60 [==============================] - 3s 56ms/step - loss: 0.4198 - accuracy: 0.8162\nEpoch 12/20\n60/60 [==============================] - 3s 56ms/step - loss: 0.4196 - accuracy: 0.8130\nEpoch 13/20\n60/60 [==============================] - 4s 59ms/step - loss: 0.4112 - accuracy: 0.8179\nEpoch 14/20\n60/60 [==============================] - 3s 54ms/step - loss: 0.4123 - accuracy: 0.8200\nEpoch 15/20\n60/60 [==============================] - 3s 55ms/step - loss: 0.4068 - accuracy: 0.8194\nEpoch 16/20\n60/60 [==============================] - 3s 56ms/step - loss: 0.3973 - accuracy: 0.8257\nEpoch 17/20\n60/60 [==============================] - 3s 54ms/step - loss: 0.3874 - accuracy: 0.8306\nEpoch 18/20\n60/60 [==============================] - 3s 54ms/step - loss: 0.3867 - accuracy: 0.8308\nEpoch 19/20\n60/60 [==============================] - 3s 57ms/step - loss: 0.3813 - accuracy: 0.8342\nEpoch 20/20\n60/60 [==============================] - 3s 53ms/step - loss: 0.3761 - accuracy: 0.8403\n","output_type":"stream"}]},{"cell_type":"code","source":"# Make the end-to-end model and test it on a custom string!\nstring_input = keras.Input(shape=(1,), dtype=\"string\")\nx = vectorizer(string_input)\npreds = model(x)\nend_to_end_model = keras.Model(string_input, preds)\n\nprobability = end_to_end_model.predict(\n    [[\"there is a wildfire in california\"]]\n)\n\nprint(probability)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T19:17:33.132251Z","iopub.execute_input":"2023-06-27T19:17:33.132656Z","iopub.status.idle":"2023-06-27T19:17:34.031132Z","shell.execute_reply.started":"2023-06-27T19:17:33.132629Z","shell.execute_reply":"2023-06-27T19:17:34.029935Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 549ms/step\n[[0.7955165]]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create submission\nsample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\noutput = []\nfor s in test_samples:\n    probability = end_to_end_model.predict([[s]])\n    output.append(1 if probability > 0.5 else 0)\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[\"target\"] = output\nsample_submission.head()\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T19:23:18.743317Z","iopub.execute_input":"2023-06-27T19:23:18.743698Z","iopub.status.idle":"2023-06-27T19:23:18.758181Z","shell.execute_reply.started":"2023-06-27T19:23:18.743670Z","shell.execute_reply":"2023-06-27T19:23:18.756279Z"},"trusted":true},"execution_count":42,"outputs":[]}]}